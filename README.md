
# ğŸ›¡ï¸ InsuraFlow

![PySpark](https://img.shields.io/badge/PySpark-Data%20Engineering-orange?style=flat-square&logo=apache-spark)
![Apache Spark](https://img.shields.io/badge/Spark-Optimized-green?style=flat-square&logo=apache-spark)
![JSON](https://img.shields.io/badge/Data-JSON-blue?style=flat-square)
![GitHub last commit](https://img.shields.io/github/last-commit/Madhugamidi14/InsuraFlow)

## ğŸš€ Project Overview

**InsuraFlow** is a full-fledged, industry-standard, PySpark-based data engineering pipeline that ingests, cleans, transforms, and prepares insurance data for analytics and visualization. Designed with modularity, scalability, and real-world inconsistencies in mind, this project is perfect for demonstrating end-to-end data engineering capabilities using big data tools.

---

Insurance datasets often contain inconsistencies, formatting issues, and require enrichment for meaningful insights. **InsuraFlow** handles:

- ğŸ§¹ **Data Cleaning**: Fixes formatting issues, handles missing values, validates and normalizes fields.
- ğŸ”„ **Data Transformation**: Enriches and reshapes the cleaned data for analytical use.
- ğŸ”— **Pipeline Orchestration**: Modular execution using Jupyter Notebooks and Papermill.
- ğŸ“ **Robust Logging**: Logs every operation using centralized logging with contextual labels (`[CLEANING]`, `[TRANSFORMATION]`).

---

## ğŸ“‚ Project Structure

```bash
InsuraFlow/
â”œâ”€â”€ input/                       # Raw input files (JSON)
â”œâ”€â”€ output/                      # Cleaned + Transformed outputs
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ cleaning/                # Data cleaning notebook
â”‚   â”‚   â””â”€â”€ data_cleaning.ipynb
â”‚   â””â”€â”€ transforming/            # Data transformation notebook
â”‚       â””â”€â”€ data_transformer.ipynb
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline.log             # Truncated and reused every run
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml              # Centralized config for file paths
â”œâ”€â”€ main.py                      # Orchestration script
â””â”€â”€ README.md                    # You're here!
```

---

## ğŸ§¼ Cleaning Highlights

- Standardizes gender, email, boolean fields
- Validates and formats date columns
- Normalizes `contact_number` to **US format** like `+1 (123) 456-7890`
- Removes `xEXT` from phone numbers (e.g. `x12345`)
- Nullifies `claim_status` if `claim_id` is missing or invalid
- Robust logging after every step

---

## ğŸ” Transformations

- Categorizes policies into **Risk Levels** (`High`, `Medium`, `Low`) based on claim amounts
- Generates `full_address` using `address + zip_code`
- Drops unnecessary fields post-cleaning (e.g., `contact_number`)

---

## ğŸ“˜ Configurable Parameters

All notebook paths and logs are driven by `config/config.yaml`:

```yaml
log_file: logs/pipeline.log

notebooks:
  cleaning:
    input: notebooks/cleaning/data_cleaning.ipynb
    output_template: notebooks/cleaning/data_cleaning_output_{timestamp}.ipynb
  transforming:
    input: notebooks/transforming/data_transformer.ipynb
    output_template: notebooks/transforming/data_transformer_output_{timestamp}.ipynb
```

---

## ğŸ§ª Tech Stack

- **PySpark**
- **Pandas**
- **Jupyter + Papermill**
- **YAML-based configuration**
- **Logging & Exception Handling**

---

## ğŸ‘¤ Author

**Sree Madhuchandra Gamidi**  
ğŸ“Œ GitHub: [@Madhugamidi14](https://github.com/Madhugamidi14)  
ğŸ”— LinkedIn: [linkedin.com/in/madhu-gamidi-31976918b](https://www.linkedin.com/in/madhu-gamidi-31976918b)

---

## ğŸ“ˆ What's Next?

- âœ… Cleaning + Transformation complete  
- ğŸ”œ Ingestion to PostgreSQL using Parquet  
- ğŸ”œ DBT transformations and modeling  
- ğŸ”œ Visualization using Superset or Metabase  
- ğŸ”œ Airflow-based orchestration  
- ğŸ”œ CI/CD + Docker packaging


---

## â­ï¸ Star This Repo

If you find **InsuraFlow** helpful, show some â¤ï¸ by starring the repo!
