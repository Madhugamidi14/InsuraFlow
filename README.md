# ðŸ›¡ï¸ InsuraFlow

![PySpark](https://img.shields.io/badge/PySpark-Data%20Engineering-orange?style=flat-square&logo=apache-spark)
![Apache Spark](https://img.shields.io/badge/Spark-Optimized-green?style=flat-square&logo=apache-spark)
![JSON](https://img.shields.io/badge/Data-JSON-blue?style=flat-square)
![Dagster](https://img.shields.io/badge/Orchestration-Dagster-purple?style=flat-square&logo=dagster)
![GitHub last commit](https://img.shields.io/github/last-commit/Madhugamidi14/InsuraFlow)

## ðŸš€ Project Overview

**InsuraFlow** is a full-fledged, industry-standard, PySpark-based data engineering pipeline that ingests, cleans, transforms, and prepares insurance data for analytics and visualization. Designed with modularity, scalability, and real-world inconsistencies in mind, this project is perfect for demonstrating end-to-end data engineering capabilities using big data tools Itâ€™s designed for modularity, fault-tolerance, and real-world data inconsistencies â€” powered by PySpark + Dagster.


---

Insurance datasets often contain inconsistencies, formatting issues, and require enrichment for meaningful insights. **InsuraFlow** handles:

- ðŸ§¹ **Cleaning**: Standardizes, nullifies, formats fields (e.g. gender, phone, dates)
- ðŸ” **Transformation**: Adds derived features like risk level and full address
- âš™ï¸ **Orchestration with Dagster**: Jupyter notebooks materialized as assets
- ðŸ““ **Notebook Automation**: Powered by Papermill
- ðŸ”§ **Central Config**: YAML-based dynamic file paths
- ðŸ§¾ **Logging**: Central log file (`logs/pipeline.log`) for all pipeline steps

---

## ðŸ“‚ Project Structure

```bash
InsuraFlow/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                # Central YAML for file paths + notebooks
â”œâ”€â”€ input/                         # âš ï¸ Ignored - Raw input files (e.g., insurance_data.json)
â”œâ”€â”€ output/                        # âš ï¸ Ignored - Cleaned & transformed output files
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ cleaning/
â”‚   â”‚   â””â”€â”€ data_cleaning.ipynb    # Cleaning logic with PySpark
â”‚   â””â”€â”€ transforming/
â”‚       â””â”€â”€ data_transformer.ipynb # Transforming logic
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline.log               # Unified log file (cleared every run)
â”œâ”€â”€ insuraflow_dag/
â”‚   â”œâ”€â”€ assets.py                  # Dagster assets (cleaning, transformation)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ start_dagster.sh           # ðŸš€ Shell script to auto-start Dagster + cleanup
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ðŸ”„ Dagster-Powered Execution

Each pipeline step is defined as a **Dagster asset** that:

- Executes the corresponding notebook via Papermill
- Passes dynamic paths from `config.yaml`
- Validates output
- Deletes temporary files automatically

```python
@asset
def run_cleaning_notebook() -> Output[str]:
    ...
    pm.execute_notebook(input_path, temp_path, parameters={...})
    ...
```

---

## ðŸ§¼ Cleaning Logic

- Standardizes:
  - `gender`, `email_address`, `smoker`, `renewal_flag`
- Formats:
  - `contact_number` â†’ US format: `+1 (555) 123-4567`
  - Dates using `to_date`
- Nullifies:
  - `claim_status` if `claim_id` is missing or invalid
- Logs:
  - Every step is logged using `[CLEANING]` prefix

---

## ðŸ” Transformation Logic

- Categorizes policies into **Risk Levels**:
  - Based on `claim_amount`
- Combines:
  - `address + zip_code` into `full_address`
- Drops:
  - Columns like `contact_number` post-cleaning
- Logs with `[TRANSFORMATION]` prefix


---

## ðŸš€ Usage

```bash

sh insuraflow_dag/start_dagster.sh
```

Then navigate to `http://localhost:3000` and **materialize the assets**!

---

## ðŸ“˜ Tech Stack

- **Apache Spark (PySpark)**
- **Dagster**
- **Papermill**
- **YAML for config**
- **Pandas (for export)**
- **VSCode + GitHub**

---

## ðŸ“ˆ Pipeline Status

- âœ… Cleaning notebook âœ…
- âœ… Transformation notebook âœ…
- âœ… Dynamic config via YAML âœ…
- âœ… GitHub integration âœ…
- âœ… Dagster orchestration âœ…
- âœ… PostgreSQL Ingestion (Supabase) âœ…
- âœ… dbt transformations âœ…
- â³ Visualization (Superset / Metabase)


---

## ðŸ‘¤ Author

**Sree Madhuchandra Gamidi**  
ðŸ“Œ GitHub: [@Madhugamidi14](https://github.com/Madhugamidi14)  
ðŸ”— LinkedIn: [linkedin.com/in/madhu-gamidi-31976918b](https://www.linkedin.com/in/madhu-gamidi-31976918b)

---

## â­ï¸ Star This Repo

If you like this project, star â­ï¸ it on GitHub!

---

## ðŸ§  Tip

> Run `git status` regularly and check `.gitignore` to avoid pushing large files like `input/`, `output/`, or `logs/`.