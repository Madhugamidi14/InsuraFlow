{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "notebooks/data_cleaning.ipynb\n",
    "Performs cleaning on insurance data using PySpark.\n",
    "Logs everything to logs/pipeline.log.\n",
    "\"\"\"\n",
    "\n",
    "# Parameters\n",
    "input_path = None\n",
    "output_path = None\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, trim, lower, when, to_date, lit\n",
    "from pyspark.sql.types import StringType\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "# ------------------ Load Config ------------------\n",
    "config_path = globals().get(\"config_path\", None)\n",
    "\n",
    "if config_path:\n",
    "    config_file = config_path\n",
    "else:\n",
    "    # fallback to default (local testing)\n",
    "    notebook_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    project_root = os.path.abspath(os.path.join(notebook_dir, \"..\", \"..\"))\n",
    "    config_file = os.path.join(project_root, \"config\", \"config.yaml\")\n",
    "\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "input_path = config[\"paths\"][\"raw_data\"]\n",
    "output_path = config[\"paths\"][\"cleaned_data\"]\n",
    "log_file = config[\"log_file\"]\n",
    "\n",
    "\n",
    "# ------------------ Logger Setup ------------------\n",
    "\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logger = logging.getLogger(\"data_cleaning\")\n",
    "if not logger.hasHandlers():\n",
    "    handler = logging.FileHandler(log_file)\n",
    "    formatter = logging.Formatter(\"[CLEANING] %(asctime)s - %(levelname)s - %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# ------------------ Spark Session ------------------\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"InsuranceCleaner\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.hadoop.io.native.lib.available\", \"false\") \\\n",
    "    .config(\"spark.speculation\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logger.info(\"Spark session started.\")\n",
    "\n",
    "# ------------------ Read JSON ------------------\n",
    "try:\n",
    "    logger.info(\"Starting the file load from input folder\")\n",
    "    df = spark.read.json(input_path)\n",
    "    logger.info(\"Successfully loaded input JSON file.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load input file: {e}\")\n",
    "    raise  # or continue gracefully\n",
    "\n",
    "# ------------------ Cleaning Functions ------------------\n",
    "def clean_string_columns(df, columns):\n",
    "    for col_name in columns:\n",
    "        df = df.withColumn(col_name, trim(col(col_name)))\n",
    "        df = df.withColumn(col_name, regexp_replace(col(col_name), r'\\s+', ' '))\n",
    "    return df\n",
    "\n",
    "def standardize_gender(df):\n",
    "    return df.withColumn(\"gender\", \n",
    "        when(lower(col(\"gender\")).isin(\"m\", \"male\"), \"Male\")\n",
    "       .when(lower(col(\"gender\")).isin(\"f\", \"female\"), \"Female\")\n",
    "       .otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "def standardize_dates(df, date_columns):\n",
    "    for col_name in date_columns:\n",
    "        df = df.withColumn(col_name, to_date(col(col_name), 'yyyy-MM-dd'))\n",
    "    return df\n",
    "\n",
    "def clean_email(df):\n",
    "    return df.withColumn(\"email_address\", lower(trim(col(\"email_address\"))))\n",
    "\n",
    "def normalize_booleans(df, bool_columns):\n",
    "    for col_name in bool_columns:\n",
    "        df = df.withColumn(col_name, \n",
    "            when(lower(col(col_name)).isin(\"yes\", \"y\", \"true\"), \"Yes\")\n",
    "           .when(lower(col(col_name)).isin(\"no\", \"n\", \"false\"), \"No\")\n",
    "           .otherwise(\"Unknown\")\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def nullify_claim_status_if_id_missing(df):\n",
    "    return df.withColumn(\n",
    "        \"claim_status\",\n",
    "        when(\n",
    "            col(\"claim_id\").isNull() |\n",
    "            (lower(col(\"claim_id\")) == \"nan\") |\n",
    "            (lower(col(\"claim_id\")) == \"null\") |\n",
    "            (col(\"claim_id\") == \"\"),\n",
    "            lit(None)\n",
    "        ).otherwise(col(\"claim_status\"))\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------ Execute Cleaning ------------------\n",
    "try:\n",
    "    logger.info(\"Started cleaning string columns\")\n",
    "    df = clean_string_columns(df, [\n",
    "        \"address\", \"agent_name\", \"claim_reason\", \"claim_status\",\n",
    "        \"customer_name\", \"education_level\", \"employment_status\",\n",
    "        \"marital_status\", \"payment_frequency\", \"policy_status\",\n",
    "        \"policy_type\", \"property_type\", \"region\", \"vehicle_type\"\n",
    "    ])\n",
    "    logger.info(\"Cleaning string columns successful\")\n",
    "\n",
    "    logger.info(\"Started standardizing gender\")\n",
    "    df = standardize_gender(df)\n",
    "    logger.info(\"Standardizing genders successful\")\n",
    "    \n",
    "    logger.info(\"Started standardizing Date columns\")\n",
    "    df = standardize_dates(df, [\n",
    "        \"claim_date\", \"date_of_birth\", \"last_payment_date\",\n",
    "        \"next_due_date\", \"policy_start_date\", \"policy_end_date\"\n",
    "    ])\n",
    "    logger.info(\"Standardizing Dates successful\")\n",
    "    \n",
    "    logger.info(\"Started cleaning EMAIL\")\n",
    "    df = clean_email(df)\n",
    "    logger.info(\"Cleaning EMAIL successfull\")\n",
    "    \n",
    "\n",
    "    \n",
    "    logger.info(\"Started normalizing booleans\")\n",
    "    df = normalize_booleans(df, [\"renewal_flag\", \"smoker\"])\n",
    "    logger.info(\"Normalizing booleans successful\")\n",
    "    \n",
    "    logger.info(\"Started nullifying claim status if claim_id is missing\")\n",
    "    df = nullify_claim_status_if_id_missing(df)\n",
    "    logger.info(\"Nullifying Claim Status successful\")\n",
    "\n",
    "    logger.info(\"Data cleaning completed successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during data cleaning: {e}\")\n",
    "    raise \n",
    "\n",
    "# ------------------ Save Cleaned Data ------------------\n",
    "try:\n",
    "    logger.info(\"Started saving the cleaned output file\")\n",
    "    output_path = output_path\n",
    "    desired_order = [\n",
    "        \"policy_id\", \"customer_id\", \"customer_name\", \"gender\", \"date_of_birth\", \"age\",\n",
    "        \"policy_type\", \"coverage_amount\", \"premium_amount\", \"payment_frequency\",\n",
    "        \"policy_start_date\", \"policy_end_date\",\n",
    "        \"claim_id\", \"claim_date\", \"claim_amount\", \"claim_reason\", \"claim_status\",\n",
    "        \"agent_id\", \"agent_name\", \"region\",\n",
    "        \"customer_income\", \"marital_status\", \"number_of_dependents\",\n",
    "        \"vehicle_type\", \"vehicle_age\",\n",
    "        \"property_type\", \"property_value\",\n",
    "        \"health_condition\", \"smoker\", \"employment_status\", \"education_level\",\n",
    "        \"policy_status\", \"last_payment_date\", \"next_due_date\", \"renewal_flag\",\n",
    "        \"contact_number\", \"email_address\", \"address\", \"zip_code\"\n",
    "    ]\n",
    "\n",
    "    # Reorder Spark DataFrame and convert to Pandas\n",
    "    logger.info(\"Re-ordering column names per the input order\")\n",
    "    existing_columns = [c for c in desired_order if c in df.columns]\n",
    "    df = df.select(*existing_columns)\n",
    "    df_pd = df.toPandas()\n",
    "    logger.info(\"Re-ordering successful\")\n",
    "\n",
    "    # Format datetime columns as string\n",
    "    logger.info(\"Formatting DateTime columns\")\n",
    "    date_columns = [\n",
    "        \"claim_date\", \"date_of_birth\", \"last_payment_date\",\n",
    "        \"next_due_date\", \"policy_start_date\", \"policy_end_date\"\n",
    "    ]\n",
    "    for col_name in date_columns:\n",
    "        if col_name in df_pd.columns:\n",
    "            df_pd[col_name] = df_pd[col_name].astype(str)\n",
    "    logger.info(\"Formatting DateTime columns successful\")\n",
    "\n",
    "    logger.info(\"Converting Age column to type:Integer\")\n",
    "    if \"age\" in df_pd.columns:\n",
    "        df_pd[\"age\"] = pd.to_numeric(df_pd[\"age\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    logger.info(\"Converted successfully\")\n",
    "\n",
    "    # Save as JSON (one line per record)\n",
    "    df_pd.to_json(output_path, orient=\"records\", lines=True)\n",
    "\n",
    "    logger.info(f\"Saving data successful. Cleaned data written to {output_path} using Pandas.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to write cleaned data: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------ Stop Spark ------------------\n",
    "spark.stop()\n",
    "logger.info(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
