{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66e35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data Engineering Prep\\Pyspark\\.venv\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "input_path = None\n",
    "output_path = None\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, concat_ws\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# ------------------ Load Config ------------------\n",
    "config_path = os.environ.get(\"INSURAFLOW_CONFIG_PATH\")\n",
    "\n",
    "if not config_path:\n",
    "    # fallback: build the path manually if not set via environment variable\n",
    "    try:\n",
    "        notebook_dir = os.getcwd()  # âœ… works in notebooks\n",
    "        project_root = os.path.abspath(os.path.join(notebook_dir, \"..\", \"..\"))\n",
    "        config_path = os.path.join(project_root, \"config\", \"config.yaml\")\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(\"Failed to resolve config path.\") from e\n",
    "\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "input_path = config[\"paths\"][\"cleaned_data\"]\n",
    "output_path = config[\"paths\"][\"transformed_data\"]\n",
    "log_file = config[\"log_file\"]\n",
    "\n",
    "\n",
    "# ------------------ Logger Setup ------------------\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "\n",
    "logger = logging.getLogger(\"data_transformation\")\n",
    "if not logger.hasHandlers():\n",
    "    handler = logging.FileHandler(log_file)\n",
    "    formatter = logging.Formatter(\"[TRANSFORMATION] %(asctime)s - %(levelname)s - %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# ------------------ Spark Session ------------------\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"InsuranceTransformer\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.hadoop.io.native.lib.available\", \"false\") \\\n",
    "    .config(\"spark.speculation\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logger.info(\"Spark session started.\")\n",
    "\n",
    "\n",
    "# ------------------ Read Cleaned JSON ------------------\n",
    "try:\n",
    "    logger.info(\"Starting file upload : cleaned_insurance_data.json\")\n",
    "    df = spark.read.json(input_path)\n",
    "    logger.info(\"Loaded cleaned insurance data.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load cleaned data: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# ------------------ Transformation Functions ------------------\n",
    "\n",
    "def categorize_risk(df):\n",
    "    return df.withColumn(\"risk_category\",\n",
    "        when(col(\"claim_amount\") > 10000, \"High\")\n",
    "       .when((col(\"claim_amount\") <= 10000) & (col(\"claim_amount\") > 1000), \"Medium\")\n",
    "       .otherwise(\"Low\")\n",
    "    )\n",
    "\n",
    "def create_full_address(df):\n",
    "    return df.withColumn(\"full_address\", concat_ws(\", \", col(\"address\"), col(\"zip_code\")))\n",
    "\n",
    "# ------------------ Drop Unwanted Columns ------------------\n",
    "try:\n",
    "    logger.info(\"Dropping contact_number column as it's no longer needed.\")\n",
    "    df = df.drop(\"contact_number\")\n",
    "    logger.info(\"Dropped contact_number column successfully.\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not drop contact_number column: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# ------------------ Execute Transformations ------------------\n",
    "try:\n",
    "    logger.info(\"Starting categorizing risk\")\n",
    "    df = categorize_risk(df)\n",
    "    logger.info(\"Categorized risk.\")\n",
    "    logger.info(\"Starting creation of full address\")\n",
    "    df = create_full_address(df)\n",
    "    logger.info(\"Created Full Address\")\n",
    "\n",
    "    logger.info(\"Transformations applied successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during transformation: {e}\")\n",
    "    raise\n",
    "\n",
    "# ------------------ Save Transformed Data ------------------\n",
    "try:\n",
    "    logger.info(\"Starting file save procedure: transformed_insurance_data.json\")\n",
    "    output_path = output_path\n",
    "\n",
    "    original_order = [\n",
    "        \"policy_id\", \"customer_id\", \"customer_name\", \"gender\", \"date_of_birth\", \"age\",\n",
    "        \"policy_type\", \"coverage_amount\", \"premium_amount\", \"payment_frequency\",\n",
    "        \"policy_start_date\", \"policy_end_date\",\n",
    "        \"claim_id\", \"claim_date\", \"claim_amount\", \"claim_reason\", \"claim_status\",\n",
    "        \"agent_id\", \"agent_name\", \"region\",\n",
    "        \"customer_income\", \"marital_status\", \"number_of_dependents\",\n",
    "        \"vehicle_type\", \"vehicle_age\",\n",
    "        \"property_type\", \"property_value\",\n",
    "        \"health_condition\", \"smoker\", \"employment_status\", \"education_level\",\n",
    "        \"policy_status\", \"last_payment_date\", \"next_due_date\", \"renewal_flag\", \"email_address\", \"address\", \"zip_code\"\n",
    "    ]\n",
    "\n",
    "    # Dynamically find new transformation columns\n",
    "    new_columns = [c for c in df.columns if c not in original_order]\n",
    "    final_order = original_order + new_columns\n",
    "\n",
    "    # Select only existing columns\n",
    "    existing_columns = [c for c in final_order if c in df.columns]\n",
    "    df = df.select(*existing_columns)\n",
    "    \n",
    "    logger.info(f\"New columns added during transformation: {new_columns}\")\n",
    "\n",
    "    df_pd = df.toPandas()\n",
    "    df_pd.to_json(output_path, orient=\"records\", lines=True)\n",
    "    logger.info(f\"Transformed data written to {output_path} using pandas.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to write transformed data: {e}\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
